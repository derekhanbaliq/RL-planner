diff --git a/gym/f110_gym/envs/f110_env.py b/gym/f110_gym/envs/f110_env.py
index fa360e9..b1e7b39 100644
--- a/gym/f110_gym/envs/f110_env.py
+++ b/gym/f110_gym/envs/f110_env.py
@@ -308,6 +308,7 @@ class F110Env(gym.Env):
             done (bool): if the simulation is done
             info (dict): auxillary information dictionary
         """
+        print("reset")
         # reset counters and data members
         self.current_time = 0.0
         self.collisions = np.zeros((self.num_agents, ))
diff --git a/pure_pursuit/f110_rlenv.py b/pure_pursuit/f110_rlenv.py
index cf67250..ea64bff 100644
--- a/pure_pursuit/f110_rlenv.py
+++ b/pure_pursuit/f110_rlenv.py
@@ -84,12 +84,11 @@ class F110Env_Continuous_Planner(gym.Env):
         # print(reward, info, self.f110.collisions)
         reward -= 1 # control cost
         if self.f110.collisions[0] == 1:
-            print("collided: ", done, info)
+            # print("collided: ", done, info)
             reward -= 1
         else:
             reward += 1
         
-
         # TODO
         # reward = self.get_reward()
 
diff --git a/pure_pursuit/ppo_continuous.py b/pure_pursuit/ppo_continuous.py
index fa56340..ad7fd49 100644
--- a/pure_pursuit/ppo_continuous.py
+++ b/pure_pursuit/ppo_continuous.py
@@ -28,7 +28,7 @@ def parse_args():
         help="if toggled, cuda will be enabled by default")
     parser.add_argument("--track", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,
         help="if toggled, this experiment will be tracked with Weights and Biases")
-    parser.add_argument("--wandb-project-name", type=str, default="cleanRL",
+    parser.add_argument("--wandb-project-name", type=str, default="f1tenth_planner",
         help="the wandb's project name")
     parser.add_argument("--wandb-entity", type=str, default=None,
         help="the entity (team) of wandb's project")
@@ -76,17 +76,6 @@ def parse_args():
     # fmt: on
     return args
 
-class DictConcatWrapper(gym.ObservationWrapper):
-    def observation(self, obs):
-        # (if dict, concatenate its elements here...)
-        new_obs = [] #np.array([])
-        if isinstance(obs, dict):
-            for i, value in enumerate(obs.values()):
-                new_obs.append(value)
-        print(new_obs)
-        return np.array(new_obs)
-
-
 def make_env(env_id, idx, capture_video, run_name, gamma):
 
     def thunk():
@@ -107,7 +96,6 @@ def make_env(env_id, idx, capture_video, run_name, gamma):
         env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
         env = gym.wrappers.NormalizeReward(env, gamma=gamma)
         env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-        # env = DictConcatWrapper(env)
         return env
 
     return thunk
@@ -123,18 +111,20 @@ class Agent(nn.Module):
     def __init__(self, envs):
         super().__init__()
         self.critic = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
+            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 512)),
             nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
+            layer_init(nn.Linear(512, 512)),
+            nn.Tanh(),
+            layer_init(nn.Linear(512, 1), std=1.0),
             nn.Tanh(),
-            layer_init(nn.Linear(64, 1), std=1.0),
         )
         self.actor_mean = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
+            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 512)),
+            nn.Tanh(),
+            layer_init(nn.Linear(512, 512)),
             nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
+            layer_init(nn.Linear(512, np.prod(envs.single_action_space.shape)), std=0.01),
             nn.Tanh(),
-            layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01),
         )
         self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(envs.single_action_space.shape)))
 
@@ -237,13 +227,15 @@ if __name__ == "__main__":
             # Only print when at least 1 env is done
             # if "final_info" not in infos:
             #     continue
+            # print(next_obs, reward, done, infos)
 
-            for info in infos:
-                if info["checkpoint_done"].all():
+            for i, info in enumerate(infos):
+                # print(next_done[i], info["checkpoint_done"])
+                if next_done[i] or info["checkpoint_done"].all():
                     print(f"global_step={global_step}, episodic_return={info['episode']['r']}")
                     writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
                     writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                    
+                    # envs.envs[i].reset()
         # bootstrap value if not done
         with torch.no_grad():
             next_value = agent.get_value(next_obs).reshape(1, -1)
