diff --git a/gym/f110_gym/envs/f110_env.py b/gym/f110_gym/envs/f110_env.py
index fa360e9..b1e7b39 100644
--- a/gym/f110_gym/envs/f110_env.py
+++ b/gym/f110_gym/envs/f110_env.py
@@ -308,6 +308,7 @@ class F110Env(gym.Env):
             done (bool): if the simulation is done
             info (dict): auxillary information dictionary
         """
+        print("reset")
         # reset counters and data members
         self.current_time = 0.0
         self.collisions = np.zeros((self.num_agents, ))
diff --git a/pure_pursuit/f110_rlenv.py b/pure_pursuit/f110_rlenv.py
index cf67250..ea64bff 100644
--- a/pure_pursuit/f110_rlenv.py
+++ b/pure_pursuit/f110_rlenv.py
@@ -84,12 +84,11 @@ class F110Env_Continuous_Planner(gym.Env):
         # print(reward, info, self.f110.collisions)
         reward -= 1 # control cost
         if self.f110.collisions[0] == 1:
-            print("collided: ", done, info)
+            # print("collided: ", done, info)
             reward -= 1
         else:
             reward += 1
         
-
         # TODO
         # reward = self.get_reward()
 
diff --git a/pure_pursuit/ppo_continuous.py b/pure_pursuit/ppo_continuous.py
index fa56340..1367161 100644
--- a/pure_pursuit/ppo_continuous.py
+++ b/pure_pursuit/ppo_continuous.py
@@ -76,17 +76,6 @@ def parse_args():
     # fmt: on
     return args
 
-class DictConcatWrapper(gym.ObservationWrapper):
-    def observation(self, obs):
-        # (if dict, concatenate its elements here...)
-        new_obs = [] #np.array([])
-        if isinstance(obs, dict):
-            for i, value in enumerate(obs.values()):
-                new_obs.append(value)
-        print(new_obs)
-        return np.array(new_obs)
-
-
 def make_env(env_id, idx, capture_video, run_name, gamma):
 
     def thunk():
@@ -107,7 +96,6 @@ def make_env(env_id, idx, capture_video, run_name, gamma):
         env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
         env = gym.wrappers.NormalizeReward(env, gamma=gamma)
         env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-        # env = DictConcatWrapper(env)
         return env
 
     return thunk
@@ -237,13 +225,15 @@ if __name__ == "__main__":
             # Only print when at least 1 env is done
             # if "final_info" not in infos:
             #     continue
+            # print(next_obs, reward, done, infos)
 
-            for info in infos:
-                if info["checkpoint_done"].all():
+            for i, info in enumerate(infos):
+                # print(next_done[i], info["checkpoint_done"])
+                if next_done[i] or info["checkpoint_done"].all():
                     print(f"global_step={global_step}, episodic_return={info['episode']['r']}")
                     writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
                     writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                    
+                    # envs.envs[i].reset()
         # bootstrap value if not done
         with torch.no_grad():
             next_value = agent.get_value(next_obs).reshape(1, -1)
