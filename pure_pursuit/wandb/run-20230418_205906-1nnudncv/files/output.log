/home/oem/.pyenv/versions/rl-planner/lib/python3.8/site-packages/wandb/sdk/lib/import_hooks.py:243: DeprecationWarning: Deprecated since Python 3.4. Use importlib.util.find_spec() instead.
  loader = importlib.find_loader(fullname, path)
/home/oem/Documents/School/ESE_615/RL-planner/gym/f110_gym/envs/base_classes.py:93: UserWarning: Chosen integrator is RK4. This is different from previous versions of the gym.
  warnings.warn(f"Chosen integrator is RK4. This is different from previous versions of the gym.")
/home/oem/.pyenv/versions/rl-planner/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/oem/.pyenv/versions/rl-planner/lib/python3.8/site-packages/gym/core.py:172: DeprecationWarning: [33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.
  deprecation(
/home/oem/.pyenv/versions/rl-planner/lib/python3.8/site-packages/gym/core.py:172: DeprecationWarning: [33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.
  deprecation(
reset
reset
reset
reset
[-0.2964149]
[1.]
[-0.14098313]
[-0.84423214]
[-0.63391036]
[-0.64305794]
[-0.6419401]
[-1.]
[-0.5839593]
[-1.]
[0.8301389]
[0.32088462]
[0.1678975]
[-1.]
[0.31357047]
[0.9179239]
[-1.]
[-1.]
[0.5456264]
[1.]
[1.]
[-1.]
[-1.]
[0.39603183]
[0.8667206]
[0.10915682]
[-0.4735514]
[-0.35377687]
[0.23920776]
[-1.]
[-0.04473186]
[1.]
[-0.30132374]
[-0.13574003]
[-0.9458244]
[0.9550439]
[-1.]
[-1.]
[0.27945575]
[1.]
[-0.5891734]
[-0.05815732]
[-0.6601426]
[-1.]
[-0.8757589]
[-0.17128846]
[0.87913316]
[1.]
[-0.08520237]
[-0.18129577]
[-0.36265865]
[-0.27276015]
[0.03369521]
[0.27145883]
[0.41020408]
[-0.02594557]
[-0.64875066]
[0.46135485]
[0.8748787]
[0.33212268]
[0.8407977]
[0.55483043]
[1.]
[-0.75295293]
[-0.7220156]
[-0.81508094]
[-1.]
[-0.44381973]
[-1.]
[-0.8475073]
[-1.]
[-0.8687981]
[0.6921247]
[1.]
[-0.8418025]
[-0.51445043]
[-0.7928247]
[-1.]
[1.]
[-1.]
[-0.55494344]
[-1.]
[0.25814956]
[1.]
[1.]
[0.69372916]
[0.6694036]
[-1.]
[-1.]
[0.67626804]
[-0.44839606]
[-0.24526608]
[1.]
[-0.00023686]
[-0.2684849]
[-0.4643452]
[-1.]
[-1.]
[-0.29732814]
[0.43469894]
[1.]
[-1.]
[-0.4031097]
[-0.15719144]
[-0.6839609]
[1.]
[-0.12747666]
[-0.8211179]
[1.]
[0.5862141]
[-0.19598141]
[-1.]
[1.]
[1.]
[1.]
[1.]
[-0.7838529]
[1.]
[-0.39159408]
[-0.9395095]
[1.]
[-0.03759734]
[-0.21985735]
[1.]
[-0.23248914]
[-0.6272965]
[0.16998082]
[-0.2157068]
[-0.09533183]
[1.]
[0.8651938]
[1.]
[-0.57089186]
[-0.01120595]
[0.3802554]
[-0.5682791]
[-0.75993645]
[1.]
[-0.33148396]
[1.]
[1.]
[-0.7315536]
[0.18340692]
[-0.32512838]
[-1.]
[0.00120794]
[0.26218793]
[-0.71113336]
[-0.49344563]
[-0.3403628]
[1.]
[-1.]
[0.456927]
[-0.44548318]
[1.]
[1.]
[0.02709095]
reset
[-0.7636997]
[-0.24316205]
[-1.]
reset
global_step=160, episodic_return=-1.6000001430511475
global_step=160, episodic_return=-1.6000001430511475
[-0.45999843]
[1.]
[-0.7442626]
reset
/home/oem/Documents/School/ESE_615/RL-planner/pure_pursuit/f110_rlenv.py:76: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  main_agent_steer_speed = np.array([[main_steering, main_speed]])
Traceback (most recent call last):
  File "/home/oem/.pyenv/versions/rl-planner/lib/python3.8/site-packages/numba/core/serialize.py", line 29, in _numba_unpickle
    def _numba_unpickle(address, bytedata, hashed):
KeyboardInterrupt
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File "ppo_continuous.py", line 222, in <module>
    next_obs, reward, done, infos = envs.step(action.cpu().numpy())
  File "/home/oem/.pyenv/versions/rl-planner/lib/python3.8/site-packages/gym/vector/vector_env.py", line 112, in step
    return self.step_wait()
  File "/home/oem/.pyenv/versions/rl-planner/lib/python3.8/site-packages/gym/vector/sync_vector_env.py", line 138, in step_wait
    observation, self._rewards[i], self._dones[i], info = env.step(action)
  File "/home/oem/.pyenv/versions/rl-planner/lib/python3.8/site-packages/gym/core.py", line 327, in step
    observation, reward, done, info = self.env.step(action)
  File "/home/oem/.pyenv/versions/rl-planner/lib/python3.8/site-packages/gym/wrappers/normalize.py", line 102, in step
    obs, rews, dones, infos = self.env.step(action)
  File "/home/oem/.pyenv/versions/rl-planner/lib/python3.8/site-packages/gym/core.py", line 314, in step
    observation, reward, done, info = self.env.step(action)
  File "/home/oem/.pyenv/versions/rl-planner/lib/python3.8/site-packages/gym/wrappers/normalize.py", line 59, in step
    obs, rews, dones, infos = self.env.step(action)
  File "/home/oem/.pyenv/versions/rl-planner/lib/python3.8/site-packages/gym/core.py", line 340, in step
    return self.env.step(self.action(action))
  File "/home/oem/.pyenv/versions/rl-planner/lib/python3.8/site-packages/gym/wrappers/record_episode_statistics.py", line 28, in step
    observations, rewards, dones, infos = super().step(action)
  File "/home/oem/.pyenv/versions/rl-planner/lib/python3.8/site-packages/gym/core.py", line 280, in step
    return self.env.step(action)
  File "/home/oem/.pyenv/versions/rl-planner/lib/python3.8/site-packages/gym/core.py", line 314, in step
    observation, reward, done, info = self.env.step(action)
  File "/home/oem/Documents/School/ESE_615/RL-planner/pure_pursuit/f110_rlenv.py", line 81, in step
    raw_obs, reward, done, info = self.f110.step(steer_speed)
  File "/home/oem/Documents/School/ESE_615/RL-planner/gym/f110_gym/envs/f110_env.py", line 270, in step
    obs = self.sim.step(action)
  File "/home/oem/Documents/School/ESE_615/RL-planner/gym/f110_gym/envs/base_classes.py", line 578, in step
    agent.update_scan(agent_scans, i)
  File "/home/oem/Documents/School/ESE_615/RL-planner/gym/f110_gym/envs/base_classes.py", line 441, in update_scan
    new_scan = self.ray_cast_agents(current_scan)
  File "/home/oem/Documents/School/ESE_615/RL-planner/gym/f110_gym/envs/base_classes.py", line 223, in ray_cast_agents
    new_scan = ray_cast(np.append(self.state[0:2], self.state[4]), new_scan, self.scan_angles, opp_vertices)
[-0.28168854]
[-0.4996711]
global_step=164, episodic_return=-1.5900001525878906
[1.]
[-1.]
[-0.2536673]
[-1.]
[0.53589183]
[-0.47068703]
[1.]
[-0.50602466]
[-0.09730037]
[1.]
[-1.]
[-0.12307381]
[-0.17602465]
[0.5162668]
reset
[-0.27487433]
[0.7635991]
global_step=180, episodic_return=-1.5500001907348633
[-1.]
[-1.]
[-1.]
[1.]
[-0.33312467]
[0.7372269]
[-1.]
[0.52456486]
[-1.]
[0.39647636]
[-0.69167936]
[-0.1823614]
[0.59804547]
[-0.20999438]
[-1.]
[0.22710392]
[0.26151335]
[0.2829535]
[0.5881063]
[0.26786995]
[-0.50620496]
[0.29217014]
[0.69359547]
[-1.]
[0.28129667]
[0.35746524]
[-0.74329966]
[-1.]
[-0.00755567]
[1.]
[-1.]
[-0.78686583]
[-0.15046632]
[-1.]
[1.]
[0.7312431]
[0.73089206]
[-0.44187525]
[0.9196331]
[0.06821774]
[-0.6141483]
[1.]
[0.6180408]
[1.]
[0.84538966]
[-0.28784004]
[0.65948856]
[-0.41988105]
[-0.5253306]
[0.07775914]
[0.432251]
[-0.28168854]
[-1.]
[-0.94796187]
[-0.7336413]
[0.3859651]
[0.15819907]
[-0.82102555]
[0.9161844]
[0.4277956]
[0.23957662]
[0.16974428]
[-0.78184015]
[0.9767067]
[1.]
[1.]
[1.]
[-0.29488337]
[0.7739288]
[-1.]
[0.77148485]
[0.41832572]
[1.]
[-1.]
[0.08132102]
[0.31117216]
[0.19876735]
[0.22880052]
[0.86815983]
[0.14076224]
[-0.00034282]
[-1.]
[-1.]
[0.27706325]
[-0.35381618]
[0.7769014]
[1.]
[1.]
[0.49509716]
[0.20411646]
[1.]
[1.]
[0.6536537]
[-0.2600233]
[-1.]
[-1.]
[0.29393724]
[-0.90458566]
[0.25629452]
[1.]
[1.]
[-0.36385828]
[-1.]
[0.02303388]
[1.]
[-0.584771]
[0.33423766]
[-0.21676455]
[-0.58484286]
[1.]
[1.]
[0.64710844]
[1.]
[-1.]
[0.29521844]
[0.85646427]
[1.]
[1.]
[0.8607886]
[-0.7166102]
[0.7605906]
[0.33458465]
[1.]
[0.09155976]
[-0.8933486]
[0.6957745]
[-0.20137835]
[0.9209157]
[-0.20454921]
[-0.53564286]
[0.7616964]
[1.]
[-0.5693237]
[1.]
[0.42301762]
[0.70254743]
[0.16526175]
reset
[0.33845013]
[1.]
[-0.3642778]
reset
global_step=320, episodic_return=-1.6000001430511475
global_step=320, episodic_return=-1.6000001430511475
[-0.5518748]
[1.]
[1.]
[1.]
[-1.]