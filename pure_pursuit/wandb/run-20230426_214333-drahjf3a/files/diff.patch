diff --git a/gym/f110_gym/envs/f110_env.py b/gym/f110_gym/envs/f110_env.py
index fa360e9..b1e7b39 100644
--- a/gym/f110_gym/envs/f110_env.py
+++ b/gym/f110_gym/envs/f110_env.py
@@ -308,6 +308,7 @@ class F110Env(gym.Env):
             done (bool): if the simulation is done
             info (dict): auxillary information dictionary
         """
+        print("reset")
         # reset counters and data members
         self.current_time = 0.0
         self.collisions = np.zeros((self.num_agents, ))
diff --git a/pure_pursuit/f110_rlenv.py b/pure_pursuit/f110_rlenv.py
index cf67250..760c22a 100644
--- a/pure_pursuit/f110_rlenv.py
+++ b/pure_pursuit/f110_rlenv.py
@@ -17,7 +17,7 @@ class F110Env_Continuous_Planner(gym.Env):
         self.T = T
         self.obs_shape = (NUM_LIDAR_SCANS + self.T * 2, 1)
         
-        map_name = 'Catalunya'  # Spielberg, example, MoscowRaceway, Catalunya -- need further tuning
+        map_name = 'levine'  # Spielberg, example, MoscowRaceway, Catalunya -- need further tuning
         try:
             map_path = os.path.abspath(os.path.join('..', 'maps', map_name))
             assert os.path.exists(map_path)
@@ -26,8 +26,11 @@ class F110Env_Continuous_Planner(gym.Env):
         self.yaml_config = yaml.load(open(map_path + '/' + map_name + '_map.yaml'), Loader=yaml.FullLoader)
 
         # load waypoints
-        csv_data = np.loadtxt(map_path + '/' + map_name + '_raceline.csv', delimiter=';', skiprows=0)
-        main_waypoints = Waypoint(csv_data - 0.2)   # process these with RL
+        # csv_data = np.loadtxt(map_path + '/' + map_name + '_raceline.csv', delimiter=';', skiprows=0)
+        #csv_data = np.loadtxt(map_path + '/' + map_name + '_centerline.csv', delimiter=',', skiprows=0)
+        csv_data = np.loadtxt(map_path + '/' + map_name + '_centerline.csv', delimiter=';', skiprows=0)
+
+        main_waypoints = Waypoint(csv_data)   # process these with RL
         opponent_waypoints = Waypoint(csv_data)
 
         # load controller
@@ -35,7 +38,7 @@ class F110Env_Continuous_Planner(gym.Env):
         self.opponent_controller = PurePursuit(opponent_waypoints)
         self.main_renderer = Renderer(main_waypoints)
         self.opponent_renderer = Renderer(opponent_waypoints)
-        self.f110 = F110Env(map=map_path + '/' + map_name + '_map', map_ext='.png', num_agents=2)
+        self.f110 = F110Env(map=map_path + '/' + map_name + '_map', map_ext='.pgm', num_agents=2)
         # steer, speed
         
         self.action_space = spaces.Box(low=-1 * np.ones((self.T, )), high=np.ones((self.T, ))) # action ==> x-offset
@@ -52,11 +55,11 @@ class F110Env_Continuous_Planner(gym.Env):
         if "seed" in kwargs:
             self.seed(kwargs["seed"])
         main_agent_init_pos = np.array([self.yaml_config['init_pos']])
-        opponent_init_pos = np.array([-2.4921703, -5.3199103, 4.1368272]) # TODO generate random starting point
+        opponent_init_pos = main_agent_init_pos + np.array([0, 1, 0]) # np.array([-2.4921703, -5.3199103, 4.1368272]) # TODO generate random starting point
         init_pos = np.vstack((main_agent_init_pos, opponent_init_pos))
         raw_obs, _, done, _ = self.f110.reset(init_pos)
-        obs = self._get_obs(raw_obs)
         self.prev_raw_obs = raw_obs
+        obs = self._get_obs(raw_obs)
         self.prev_obs = obs
         return obs
     
@@ -68,28 +71,36 @@ class F110Env_Continuous_Planner(gym.Env):
         action: nd.array => x-offset + original traj of length (self.T, 1)
         """
         # TODO: spline points of horizon T
-        betterPoint = action + self.prev_obs[-2:, :]
+        # print("action: ", action)
+        # betterPoint = action + self.prev_obs[-2:, :]
+        R = lambda theta: np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])
+        axis = np.array([0, 1]).reshape(-1, 1)
+        rotated_offset = R(self.prev_raw_obs['poses_theta'][0]) @ axis * action
+        
+
+        # betterPoint = self.prev_obs[-2:, :] + 
 
-        main_speed, main_steering = self.main_controller.control_to_point(self.prev_raw_obs, betterPoint, self.main_controller.closest_index, agent=1)
+        main_speed, main_steering = self.main_controller.control(obs=self.prev_raw_obs, agent=1, offset=rotated_offset[:, 0])
+        # main_speed, main_steering = self.main_controller.control_to_point(self.prev_raw_obs, betterPoint, self.main_controller.closest_index, agent=1)
         opponent_speed, opponent_steering = self.opponent_controller.control(obs=self.prev_raw_obs, agent=2)
         main_agent_steer_speed = np.array([[main_steering, main_speed]])
         opponent_steer_speed = np.array([[opponent_steering, opponent_speed]])
 
         steer_speed = np.vstack((main_agent_steer_speed, opponent_steer_speed))
+        # print(steer_speed)
 
         raw_obs, reward, done, info = self.f110.step(steer_speed)
+        self.prev_raw_obs = raw_obs
         obs = self._get_obs(raw_obs)
         self.prev_obs = obs
-        self.prev_raw_obs = raw_obs
         # print(reward, info, self.f110.collisions)
         reward -= 1 # control cost
         if self.f110.collisions[0] == 1:
-            print("collided: ", done, info)
+            # print("collided: ", done, info)
             reward -= 1
         else:
             reward += 1
         
-
         # TODO
         # reward = self.get_reward()
 
@@ -101,10 +112,21 @@ class F110Env_Continuous_Planner(gym.Env):
         obs = np.zeros(self.obs_shape)
         if isinstance(raw_obs, tuple):
             raw_obs = raw_obs[0]
-        obs[:NUM_LIDAR_SCANS, :] = raw_obs['scans'][0].reshape(-1, 1)
-
-        targetPoint, _  = self.main_controller.get_target_waypoint(raw_obs, agent=1)
-
+        # print('agent 1: ', raw_obs['poses_x'][0], raw_obs['poses_y'][0], np.rad2deg(raw_obs['poses_theta'][0]))
+        # print('agent 2: ', raw_obs['poses_x'][1], raw_obs['poses_y'][1])
+        scans = raw_obs['scans'][0].reshape(-1, 1)
+        # negative_distance, positive_distance = 0, 0
+        # angle_increment = 360/NUM_LIDAR_SCANS
+        # angles = np.deg2rad(np.arange(0, 360, angle_increment))
+        # sin_angles, cos_angles = np.sin(angles), np.cos(angles)
+        # negative_distance = scans[540]
+        # positive_distance = scans[0]
+        # print("distance: ", negative_distance, positive_distance)
+        # print("sum: ", negative_distance + positive_distance)
+        
+        obs[:NUM_LIDAR_SCANS, :] = scans
+        targetPoint, idx  = self.main_controller.get_target_waypoint(self.prev_raw_obs, agent=1)
+        # print(targetPoint)
         obs[-2:, :] = targetPoint.reshape(-1, 1)
         return obs
     
diff --git a/pure_pursuit/main.py b/pure_pursuit/main.py
index 9b25483..e6c2b9e 100644
--- a/pure_pursuit/main.py
+++ b/pure_pursuit/main.py
@@ -2,6 +2,7 @@ import gym
 import numpy as np
 import yaml
 import os
+import os.path as osp
 from argparse import Namespace
 
 from pure_pursuit import PurePursuit, Waypoint
@@ -13,21 +14,30 @@ def main():
     method_name = 'pure_pursuit'
 
     # load map & yaml
-    map_name = 'Catalunya'  # Spielberg, example, MoscowRaceway, Catalunya -- need further tuning
-    map_path = os.path.abspath(os.path.join('..', 'maps', map_name))
+    map_name = 'levine'  # Spielberg, example, MoscowRaceway, Catalunya -- need further tuning
+    try:
+        map_path = os.path.abspath(os.path.join('..', 'maps', map_name))
+        assert osp.exists(map_path)
+    except:
+        map_path = os.path.abspath(os.path.join('maps', map_name))
     yaml_config = yaml.load(open(map_path + '/' + map_name + '_map.yaml'), Loader=yaml.FullLoader)
 
     # load waypoints
-    csv_data = np.loadtxt(map_path + '/' + map_name + '_raceline.csv', delimiter=';', skiprows=0)
-    main_waypoints = Waypoint(csv_data - 0.2)   # process these with RL
-    opponent_waypoints = Waypoint(csv_data)
+    # csv_data = np.loadtxt(map_path + '/' + map_name + '_raceline.csv', delimiter=';', skiprows=0)
+    # csv_data = np.loadtxt(map_path + '/' + map_name + '_centerline.csv', delimiter=',', skiprows=0)
+    csv_data = np.loadtxt(map_path + '/' + map_name + '_centerline.csv', delimiter=';', skiprows=0)
+
+    main_waypoints = Waypoint(csv_data, centerline=False)   # process these with RL
+    opponent_waypoints = Waypoint(csv_data, centerline=False)
 
     # load controller
     main_controller = PurePursuit(main_waypoints)
     opponent_controller = PurePursuit(opponent_waypoints)
 
     # create env & init
-    env = gym.make('f110_gym:f110-v0', map=map_path + '/' + map_name + '_map', map_ext='.png', num_agents=2)
+    from f110_gym.envs.f110_env import F110Env
+    map_ext = ".pgm" # ".png"
+    env = F110Env(map=map_path + '/' + map_name + '_map', map_ext=map_ext, num_agents=2) # gym.make('f110_gym:f110-v0', map=map_path + '/' + map_name + '_map', map_ext='.png', num_agents=2)
     main_renderer = Renderer(main_waypoints)
     opponent_renderer = Renderer(opponent_waypoints)
     env.add_render_callback(main_renderer.render_waypoints)
@@ -35,10 +45,11 @@ def main():
 
     # init
     main_agent_init_pos = np.array([yaml_config['init_pos']])
-    opponent_init_pos = np.array([-2.4921703, -5.3199103, 4.1368272])   # random starting point?
+    opponent_init_pos = main_agent_init_pos - np.array([0, 1, 0]) # np.array([[2.51, 2.29, 1.58]]) # [-2.4921703, -5.3199103, 4.1368272])   # random starting point?
 
     init_pos = np.vstack((main_agent_init_pos, opponent_init_pos))
     obs, _, done, _ = env.reset(init_pos)
+    env.render(mode='human')
     lap_time = 0.0
 
     while not done:
@@ -49,6 +60,11 @@ def main():
         currY = obs['poses_y'][0]
         currTH = obs['poses_theta'][0]
         currV = obs['linear_vels_x'][0]
+        # print('agent 1: ', currX, currY)
+        # print('agent 2: ', obs['poses_x'][1], obs['poses_y'][1])
+
+        # currX = 
+        # currY = 
 
         # TODO: get the current waypoint -> RL planner -> new waypoints
         # wp = controller._get_current_waypoint(controller.waypoints, 5, position, pose_theta)
@@ -68,10 +84,11 @@ def main():
         opponent_steer_speed = np.array([[opponent_steering, opponent_speed]])
 
         steer_speed = np.vstack((main_agent_steer_speed, opponent_steer_speed))
+        # print(steer_speed)
         obs, time_step, done, _ = env.step(steer_speed)
 
         lap_time += time_step
-        env.render(mode='human_fast')
+        env.render(mode='human')
 
     print('Sim elapsed time:', lap_time)
 
diff --git a/pure_pursuit/ppo_continuous.py b/pure_pursuit/ppo_continuous.py
index fa56340..e5ecf9b 100644
--- a/pure_pursuit/ppo_continuous.py
+++ b/pure_pursuit/ppo_continuous.py
@@ -28,7 +28,7 @@ def parse_args():
         help="if toggled, cuda will be enabled by default")
     parser.add_argument("--track", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,
         help="if toggled, this experiment will be tracked with Weights and Biases")
-    parser.add_argument("--wandb-project-name", type=str, default="cleanRL",
+    parser.add_argument("--wandb-project-name", type=str, default="f1tenth_planner",
         help="the wandb's project name")
     parser.add_argument("--wandb-entity", type=str, default=None,
         help="the entity (team) of wandb's project")
@@ -76,17 +76,6 @@ def parse_args():
     # fmt: on
     return args
 
-class DictConcatWrapper(gym.ObservationWrapper):
-    def observation(self, obs):
-        # (if dict, concatenate its elements here...)
-        new_obs = [] #np.array([])
-        if isinstance(obs, dict):
-            for i, value in enumerate(obs.values()):
-                new_obs.append(value)
-        print(new_obs)
-        return np.array(new_obs)
-
-
 def make_env(env_id, idx, capture_video, run_name, gamma):
 
     def thunk():
@@ -107,7 +96,6 @@ def make_env(env_id, idx, capture_video, run_name, gamma):
         env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
         env = gym.wrappers.NormalizeReward(env, gamma=gamma)
         env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-        # env = DictConcatWrapper(env)
         return env
 
     return thunk
@@ -123,18 +111,20 @@ class Agent(nn.Module):
     def __init__(self, envs):
         super().__init__()
         self.critic = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
+            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 512)),
             nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
+            layer_init(nn.Linear(512, 512)),
+            nn.Tanh(),
+            layer_init(nn.Linear(512, 1), std=1.0),
             nn.Tanh(),
-            layer_init(nn.Linear(64, 1), std=1.0),
         )
         self.actor_mean = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
+            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 512)),
+            nn.Tanh(),
+            layer_init(nn.Linear(512, 512)),
             nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
+            layer_init(nn.Linear(512, np.prod(envs.single_action_space.shape)), std=0.01),
             nn.Tanh(),
-            layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01),
         )
         self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(envs.single_action_space.shape)))
 
@@ -232,18 +222,20 @@ if __name__ == "__main__":
             next_obs, reward, done, infos = envs.step(action.cpu().numpy())
             rewards[step] = torch.tensor(reward).to(device).view(-1)
             next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)
-            # envs.envs[0].render(mode='human_fast')
+            # envs.envs[0].render(mode='human')
 
             # Only print when at least 1 env is done
             # if "final_info" not in infos:
             #     continue
+            # print(next_obs, reward, done, infos)
 
-            for info in infos:
-                if info["checkpoint_done"].all():
+            for i, info in enumerate(infos):
+                # print(next_done[i], info["checkpoint_done"])
+                if next_done[i] or info["checkpoint_done"].all():
                     print(f"global_step={global_step}, episodic_return={info['episode']['r']}")
                     writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
                     writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                    
+                    # envs.envs[i].reset()
         # bootstrap value if not done
         with torch.no_grad():
             next_value = agent.get_value(next_obs).reshape(1, -1)
diff --git a/pure_pursuit/pure_pursuit.py b/pure_pursuit/pure_pursuit.py
index b40d140..d84215a 100644
--- a/pure_pursuit/pure_pursuit.py
+++ b/pure_pursuit/pure_pursuit.py
@@ -4,12 +4,17 @@ import os
 
 
 class Waypoint:
-    def __init__(self, csv_data=None):
-        self.x = csv_data[:, 1]
-        self.y = csv_data[:, 2]
-        self.v = csv_data[:, 5]
-        self.θ = csv_data[:, 3]  # coordinate matters!f -- but pp doesn't use θ
-        self.γ = csv_data[:, 4]
+    def __init__(self, csv_data=None, centerline=False):
+        if centerline:
+            self.x = csv_data[:, 0]
+            self.y = csv_data[:, 0]
+            self.v = np.ones_like(self.y)
+        else:
+            self.x = csv_data[:, 1]
+            self.y = csv_data[:, 2]
+            self.v = csv_data[:, 5]
+            self.θ = csv_data[:, 3]  # coordinate matters!f -- but pp doesn't use θ
+            self.γ = csv_data[:, 4]
 
 
 class PurePursuit:
@@ -53,9 +58,11 @@ class PurePursuit:
 
         # Find target point
         targetPoint, target_point_index = self.get_closest_point_beyond_lookahead_dist(self.L)
+        # print(f"agent num: {agent} at {target_point_index}")
+        self.targetPoint = targetPoint
         return targetPoint, target_point_index
 
-    def control(self, obs, agent):
+    def control(self, obs, agent, offset=np.zeros((2,))):
         # Get current pose
         self.currX = obs['poses_x'][agent - 1]
         self.currY = obs['poses_y'][agent - 1]
@@ -67,7 +74,10 @@ class PurePursuit:
 
         # Find target point
         targetPoint, target_point_index = self.get_closest_point_beyond_lookahead_dist(self.L)
-
+        if agent == 1:
+            assert (targetPoint == self.targetPoint).all(), f"{targetPoint}, {self.targetPoint}"
+        targetPoint += offset # np.array([offset, 0])
+        # print(f"agent num: {agent} at {target_point_index}")
         # calculate steering angle / curvature
         waypoint_y = np.dot(np.array([np.sin(-obs['poses_theta'][agent - 1]), np.cos(-obs['poses_theta'][agent - 1])]),
                             targetPoint - np.array([self.currX, self.currY]))
diff --git a/pure_pursuit/runs/F1Tenth-Planner__ppo_continuous__1__1681748607/events.out.tfevents.1681748607.g.16934.0 b/pure_pursuit/runs/F1Tenth-Planner__ppo_continuous__1__1681748607/events.out.tfevents.1681748607.g.16934.0
deleted file mode 100644
index 6f4bbf6..0000000
Binary files a/pure_pursuit/runs/F1Tenth-Planner__ppo_continuous__1__1681748607/events.out.tfevents.1681748607.g.16934.0 and /dev/null differ
diff --git a/pure_pursuit/runs/F1Tenth-Planner__ppo_continuous__1__1681748810/events.out.tfevents.1681748810.g.17913.0 b/pure_pursuit/runs/F1Tenth-Planner__ppo_continuous__1__1681748810/events.out.tfevents.1681748810.g.17913.0
deleted file mode 100644
index 18ef9bd..0000000
Binary files a/pure_pursuit/runs/F1Tenth-Planner__ppo_continuous__1__1681748810/events.out.tfevents.1681748810.g.17913.0 and /dev/null differ
diff --git a/pure_pursuit/runs/F1Tenth-Planner__ppo_continuous__1__1681749466/events.out.tfevents.1681749466.g.20267.0 b/pure_pursuit/runs/F1Tenth-Planner__ppo_continuous__1__1681749466/events.out.tfevents.1681749466.g.20267.0
deleted file mode 100644
index d7ab497..0000000
Binary files a/pure_pursuit/runs/F1Tenth-Planner__ppo_continuous__1__1681749466/events.out.tfevents.1681749466.g.20267.0 and /dev/null differ
diff --git a/setup.py b/setup.py
index f70477c..33b268c 100644
--- a/setup.py
+++ b/setup.py
@@ -6,7 +6,7 @@ setup(name='f110_gym',
       author_email='billyzheng.bz@gmail.com',
       url='https://f1tenth.org',
       package_dir={'': 'gym'},
-      install_requires=['gym==0.19.0',
+      install_requires=['gym==0.23.1', #0.19.0
                         'numpy<=1.22.0,>=1.18.0',
                         'Pillow>=9.0.1',
                         'scipy>=1.7.3',
