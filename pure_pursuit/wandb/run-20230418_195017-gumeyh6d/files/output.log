/home/oem/.pyenv/versions/rl-planner/lib/python3.8/site-packages/wandb/sdk/lib/import_hooks.py:243: DeprecationWarning: Deprecated since Python 3.4. Use importlib.util.find_spec() instead.
  loader = importlib.find_loader(fullname, path)
/home/oem/Documents/School/ESE_615/RL-planner/gym/f110_gym/envs/base_classes.py:93: UserWarning: Chosen integrator is RK4. This is different from previous versions of the gym.
  warnings.warn(f"Chosen integrator is RK4. This is different from previous versions of the gym.")
/home/oem/.pyenv/versions/rl-planner/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/oem/.pyenv/versions/rl-planner/lib/python3.8/site-packages/gym/core.py:172: DeprecationWarning: [33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.
  deprecation(
/home/oem/.pyenv/versions/rl-planner/lib/python3.8/site-packages/gym/core.py:172: DeprecationWarning: [33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.
  deprecation(
reset
reset
reset
reset
/home/oem/Documents/School/ESE_615/RL-planner/pure_pursuit/f110_rlenv.py:75: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  main_agent_steer_speed = np.array([[main_steering, main_speed]])
reset
reset
reset
global_step=160, episodic_return=-1.6000001430511475
global_step=160, episodic_return=-1.6000001430511475
global_step=160, episodic_return=-1.6000001430511475
reset
global_step=180, episodic_return=-1.5500001907348633
reset
reset
global_step=320, episodic_return=-1.6000001430511475
global_step=320, episodic_return=-1.6000001430511475
reset
global_step=332, episodic_return=-1.570000171661377
reset
global_step=340, episodic_return=-1.6000001430511475
reset
global_step=480, episodic_return=-1.6000001430511475
reset
global_step=484, episodic_return=-1.5900001525878906
reset
global_step=496, episodic_return=-1.5900001525878906
reset
global_step=504, episodic_return=-1.5900001525878906
reset
global_step=644, episodic_return=-1.5900001525878906
reset
global_step=648, episodic_return=-1.5900001525878906
reset
global_step=660, episodic_return=-1.5900001525878906
reset
global_step=664, episodic_return=-1.6000001430511475
reset
global_step=808, episodic_return=-1.5900001525878906
reset
global_step=812, episodic_return=-1.5900001525878906
reset
global_step=824, episodic_return=-1.5900001525878906
reset
global_step=828, episodic_return=-1.5900001525878906
reset
reset
global_step=972, episodic_return=-1.5900001525878906
global_step=972, episodic_return=-1.6000001430511475
reset
global_step=984, episodic_return=-1.6000001430511475
reset
global_step=992, episodic_return=-1.5900001525878906
reset
reset
Traceback (most recent call last):
  File "/home/oem/.pyenv/versions/rl-planner/lib/python3.8/site-packages/numba/core/serialize.py", line 29, in _numba_unpickle
    def _numba_unpickle(address, bytedata, hashed):
KeyboardInterrupt
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File "ppo_continuous.py", line 220, in <module>
    next_obs, reward, done, infos = envs.step(action.cpu().numpy())
  File "/home/oem/.pyenv/versions/rl-planner/lib/python3.8/site-packages/gym/vector/vector_env.py", line 112, in step
    return self.step_wait()
  File "/home/oem/.pyenv/versions/rl-planner/lib/python3.8/site-packages/gym/vector/sync_vector_env.py", line 138, in step_wait
    observation, self._rewards[i], self._dones[i], info = env.step(action)
  File "/home/oem/.pyenv/versions/rl-planner/lib/python3.8/site-packages/gym/core.py", line 327, in step
    observation, reward, done, info = self.env.step(action)
  File "/home/oem/.pyenv/versions/rl-planner/lib/python3.8/site-packages/gym/wrappers/normalize.py", line 102, in step
    obs, rews, dones, infos = self.env.step(action)
  File "/home/oem/.pyenv/versions/rl-planner/lib/python3.8/site-packages/gym/core.py", line 314, in step
    observation, reward, done, info = self.env.step(action)
  File "/home/oem/.pyenv/versions/rl-planner/lib/python3.8/site-packages/gym/wrappers/normalize.py", line 59, in step
    obs, rews, dones, infos = self.env.step(action)
  File "/home/oem/.pyenv/versions/rl-planner/lib/python3.8/site-packages/gym/core.py", line 340, in step
    return self.env.step(self.action(action))
  File "/home/oem/.pyenv/versions/rl-planner/lib/python3.8/site-packages/gym/wrappers/record_episode_statistics.py", line 28, in step
    observations, rewards, dones, infos = super().step(action)
  File "/home/oem/.pyenv/versions/rl-planner/lib/python3.8/site-packages/gym/core.py", line 280, in step
    return self.env.step(action)
  File "/home/oem/.pyenv/versions/rl-planner/lib/python3.8/site-packages/gym/core.py", line 314, in step
    observation, reward, done, info = self.env.step(action)
  File "/home/oem/Documents/School/ESE_615/RL-planner/pure_pursuit/f110_rlenv.py", line 80, in step
    raw_obs, reward, done, info = self.f110.step(steer_speed)
  File "/home/oem/Documents/School/ESE_615/RL-planner/gym/f110_gym/envs/f110_env.py", line 270, in step
    obs = self.sim.step(action)
  File "/home/oem/Documents/School/ESE_615/RL-planner/gym/f110_gym/envs/base_classes.py", line 578, in step
    agent.update_scan(agent_scans, i)
  File "/home/oem/Documents/School/ESE_615/RL-planner/gym/f110_gym/envs/base_classes.py", line 441, in update_scan
    new_scan = self.ray_cast_agents(current_scan)
global_step=1136, episodic_return=-1.5900001525878906
global_step=1136, episodic_return=-1.5900001525878906
reset
global_step=1144, episodic_return=-1.6000001430511475
reset
global_step=1152, episodic_return=-1.6000001430511475
reset
global_step=1296, episodic_return=-1.6000001430511475
reset
global_step=1300, episodic_return=-1.5900001525878906
reset
global_step=1304, episodic_return=-1.6000001430511475
reset
global_step=1316, episodic_return=-1.5900001525878906
reset
global_step=1456, episodic_return=-1.6000001430511475
reset
global_step=1464, episodic_return=-1.5900001525878906
reset
global_step=1468, episodic_return=-1.5900001525878906
reset
global_step=1476, episodic_return=-1.6000001430511475
reset
global_step=1616, episodic_return=-1.6000001430511475
reset
global_step=1624, episodic_return=-1.6000001430511475
reset
global_step=1632, episodic_return=-1.5900001525878906
reset
global_step=1640, episodic_return=-1.5900001525878906